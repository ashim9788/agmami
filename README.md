# Federated Learning with Byzantine-Robust Aggregation (CPU + Joblib)

This repository implements a **federated learning framework** in PyTorch with support for **Byzantine (faulty or malicious) clients** and multiple **robust aggregation strategies**.
The system is intentionally designed for **CPU-based parallel client training** using `joblib`, ensuring correctness, reproducibility, and multiprocessing safety.

## ðŸ“ Project Structure
.
â”œâ”€â”€ main.py                  # Entry point (training loop)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/               # Training client data (JSON)
â”‚   â””â”€â”€ test/                # Test client data (JSON)
â””â”€â”€ federated/
    â”œâ”€â”€ config.py            # Experiment-level configuration
    â”œâ”€â”€ optim_config.py      # Optimizer definitions
    â”œâ”€â”€ fedbase.py           # Server and Clients classes
    â”œâ”€â”€ client.py            # Client-side local training logic
    â”œâ”€â”€ aggregation.py       # Aggregation algorithms
    â”œâ”€â”€ autogm.py            # Auto-GM implementation
    â”œâ”€â”€ model_definition.py  # CNN model architecture
    â””â”€â”€ data_utils.py        # Dataset loading and caching
---

## ðŸ” Key Features
* **Federated Learning (FL)** with serverâ€“client architecture
* **Byzantine fault simulation**
  * Data poisoning
  * Model corruption
  * Noise-based attacks
* **Robust aggregation algorithms**
  * FedAvg
  * Coordinate-wise Median
  * Trimmed Mean
  * Krum
  * FedMI
  * aGMaMI (Auto-GM + Mutual Information)
* **CPU-safe parallel client training** using `joblib`
* **Clean experiment configuration** via dataclasses
* **Reproducible experiments** (fixed seeds)
* FEMNIST-style CNN model (62 classes)
---

## âš™ï¸ Configuration
### Experiment Configuration
Global experiment parameters are defined in: federated/config.py
---
### Optimizers
Defined in: federated/optim_config.py
---

## ðŸ§  Design Choice: CPU + Joblib
This project intentionally avoids GPU-based multiprocessing because:
* CUDA contexts are **not fork-safe**
* `joblib` uses process-based parallelism
* CPU training guarantees:
  * correctness
  * reproducibility
  * stable execution
> ðŸ’¡ GPU support can be added later using `torch.multiprocessing`
---

## ðŸ§¯ Attack Models Supported
Configured when creating clients in `main.py`:

| Attack Type | Description              |
| ----------- | ------------------------ |
| `data`      | Label corruption         |
| `model`     | Randomized model weights |
| `noise`     | Additive Gaussian noise  |

---

## ðŸ“Š Aggregation Methods
Implemented in: federated/aggregation.py
* **FedAvg**
* **Coordinate-wise Median**
* **Trimmed Mean**
* **Krum**
* **FedMI**
* **aGMaMI** (Auto-GM + MI-based scoring)
These methods are designed to tolerate Byzantine or malicious client updates.
---

## ðŸ“ˆ Output and Logging
During training, the system reports:
* communication round index
* number of Byzantine clients
* global model accuracy
* global model loss

The framework is easily extensible to:
* save checkpoints
* log metrics to disk
* generate plots
---

## ðŸ”¬ Intended Use
This codebase is intended for:
* federated learning research
* Byzantine-robust aggregation experiments
* academic projects, theses, or papers
* reproducible experimental studies

It is **not** intended as a production FL system.
---

## ðŸ“œ License
This project is intended for **research and educational use**.
---
